{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "LOVdxGbx8wzs"
   },
   "source": [
    "# Richly Activated Graph Convolutional Network on HRI dataset\n",
    "\n",
    "We define a custom dataset that will be used to create training/test datasets. This will also help to create a DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TC06Hli28wzx",
    "ExecuteTime": {
     "end_time": "2023-08-23T12:53:01.922261700Z",
     "start_time": "2023-08-23T12:53:01.829642700Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "action_class = {'A001': 'Stop', 'A002': 'Go Right', 'A003': 'Go Left', 'A004': 'Come Here', 'A005': 'Follow me',\n",
    "                'A006': 'Go Away', 'A007': 'Agree', 'A008': 'Disagree', 'A009': 'Go there', 'A010': 'Get Attention',\n",
    "                'A011': 'Be Quiet', 'A012': 'Dont Know', 'A013': 'Turn Around', 'A014': 'Take This',\n",
    "                'A015': 'Pick Up', 'A016': 'Standing Still', 'A017': 'Being Seated', 'A018': 'Walking Towards',\n",
    "                'A019': 'Walking Away', 'A020': 'Talking on Phone'}\n",
    "lab_to_num = {'Stop': 0, 'Go Right': 1, 'Go Left': 2, 'Come Here': 3, 'Follow me': 4, 'Go Away': 5, 'Agree': 6, 'Disagree': 7,\n",
    "              'Go there': 8, 'Get Attention': 9, 'Be Quiet': 10, 'Dont Know': 11, 'Turn Around': 12, 'Take This': 13,\n",
    "              'Pick Up': 14, 'Standing Still': 15, 'Being Seated': 16, 'Walking Towards': 17, 'Walking Away': 18,\n",
    "              'Talking on Phone': 19}\n",
    "joint_dict = {'Nose': 0, 'LEye': 1, 'REye': 2, 'LEar': 3, 'REar': 4, 'LShoulder': 5, 'RShoulder': 6, 'LElbow': 7,\n",
    "              'RElbow': 8, 'LWrist': 9, 'RWrist': 10, 'LHip': 11, 'RHip': 12, 'LKnee': 13, 'RKnee': 14,\n",
    "              'LAnkle': 15, 'RAnkle': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35r5TAJm9JCC",
    "outputId": "bfc1f22f-2286-417c-ac12-eb89650e50ba",
    "ExecuteTime": {
     "end_time": "2023-08-23T12:53:01.925258600Z",
     "start_time": "2023-08-23T12:53:01.849586700Z"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "labels_to_learn = ['Stop', 'Standing Still', 'Follow me']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T12:53:01.925258600Z",
     "start_time": "2023-08-23T12:53:01.864650400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "F88uKzCx8wzz"
   },
   "source": [
    "Next we will define the RA-GCN. This is based on the original RA-GCN implementation from:\n",
    "Song, Yi-Fan, Zhang Zhang, and Liang Wang. \"Richly activated graph convolutional network for action recognition with incomplete skeletons.\" 2019 IEEE International Conference on Image Processing (ICIP). IEEE, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "RKMc4IZA8wz2",
    "ExecuteTime": {
     "end_time": "2023-08-23T12:53:01.925258600Z",
     "start_time": "2023-08-23T12:53:01.884775700Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class NTU(Dataset):\n",
    "    def __init__(self, path, train, train_split, data_shape, transform=None, mode='default'):\n",
    "\n",
    "        self.train = train\n",
    "        self.path = path\n",
    "        self.maxC, self.maxT, self.maxV, self.maxM = data_shape\n",
    "        self.transform = transform\n",
    "        self.files_in = []\n",
    "        self.files = []\n",
    "\n",
    "        for dirpath, dirnames, filenames in os.walk(self.path):\n",
    "          self.files_in.append(filenames)\n",
    "\n",
    "        self.files_in = self.files_in[1:]\n",
    "\n",
    "        # Remove all data except labels in labels_to_learn\n",
    "        for dic in range(len(self.files_in)):\n",
    "          for file in self.files_in[dic]:\n",
    "            lab = action_class[file[:4]]\n",
    "            if lab in labels_to_learn:\n",
    "              self.files.append(file)\n",
    "                    \n",
    "        random.shuffle(self.files)\n",
    "        \n",
    "        if mode == 'default':\n",
    "            if self.train == 'train':\n",
    "              self.files = self.files[:int(len(self.files) * train_split)]\n",
    "            elif self.train == 'eval':\n",
    "              self.files = self.files[int(len(self.files) * train_split):]\n",
    "        elif mode == 'cross-subject':\n",
    "            if self.train == 'train':\n",
    "                self.files = [file for file in self.files if int(file[6:8]) < 18]\n",
    "            elif self.train == 'eval':\n",
    "                self.files = [file for file in self.files if int(file[6:8]) >= 18]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.files[idx].strip()\n",
    "\n",
    "        data = np.zeros((self.maxC, self.maxT, self.maxV, self.maxM))\n",
    "        location = np.zeros((2, self.maxT, self.maxV, self.maxM))\n",
    "        with open(self.path + file_name[4:8] + '/' + file_name, 'r') as fr:\n",
    "            frame_num = int(fr.readline())\n",
    "            label = labels_to_learn.index(action_class[file_name[:4]])\n",
    "            for frame in range(frame_num):\n",
    "                if frame >= self.maxT:\n",
    "                    break\n",
    "                joint_num = int(fr.readline())\n",
    "                for joint in range(joint_num):\n",
    "                    v = fr.readline().split(' ')\n",
    "                    if joint < self.maxV:\n",
    "                        data[0,frame,joint,0] = float(v[1])\n",
    "                        data[1,frame,joint,0] = float(v[2])\n",
    "                        # [2,frame,joint,0] = float(v[3])\n",
    "                        location[0,frame,joint,0] = float(v[4])\n",
    "                        location[1,frame,joint,0] = float(v[5])\n",
    "\n",
    "        if frame_num <= self.maxT:\n",
    "            data = data[:,:self.maxT,:,:]\n",
    "        else:\n",
    "            s = frame_num // self.maxT\n",
    "            r = random.randint(0, frame_num - self.maxT * s)\n",
    "            new_data = np.zeros((self.maxC, self.maxT, self.maxV, self.maxM))\n",
    "            for i in range(self.maxT):\n",
    "                new_data[:,i,:,:] = data[:,r+s*i,:,:]\n",
    "            data = new_data\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        data = torch.from_numpy(data).float()\n",
    "        location = torch.from_numpy(location).float()\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "        return data, location, label, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7gOuwi5p8wz6",
    "ExecuteTime": {
     "end_time": "2023-08-23T12:53:01.926258500Z",
     "start_time": "2023-08-23T12:53:01.903313300Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class RA_GCN(nn.Module):\n",
    "    def __init__(self, data_shape, num_class, A, drop_prob, gcn_kernel_size, model_stream):\n",
    "        super().__init__()\n",
    "\n",
    "        C, T, V, M = data_shape\n",
    "        self.register_buffer('A', A)\n",
    "\n",
    "        # baseline\n",
    "        self.stgcn_stream = nn.ModuleList((\n",
    "            ST_GCN(data_shape, num_class, A, drop_prob, gcn_kernel_size)\n",
    "            for _ in range(model_stream)\n",
    "        ))\n",
    "\n",
    "        # mask\n",
    "        self.mask_stream = nn.ParameterList([\n",
    "            nn.Parameter(torch.ones(T * V))\n",
    "            for _ in range(model_stream)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        # multi stream\n",
    "        out = 0\n",
    "        feature = []\n",
    "        for stgcn, mask in zip(self.stgcn_stream, self.mask_stream):\n",
    "            x = inp\n",
    "\n",
    "            # mask\n",
    "            N, C, T, V, M = x.shape\n",
    "            x = x.view(N, C, -1)\n",
    "            x = x * mask[None,None,:]\n",
    "            x = x.view(N, C, T, V, M)\n",
    "\n",
    "            # baseline\n",
    "            temp_out, temp_feature = stgcn(x)\n",
    "\n",
    "            # output\n",
    "            out += temp_out\n",
    "            feature.append(temp_feature)\n",
    "        return out, feature\n",
    "\n",
    "\n",
    "class ST_GCN(nn.Module):\n",
    "    def __init__(self, data_shape, num_class, A, drop_prob, gcn_kernel_size):\n",
    "        super().__init__()\n",
    "\n",
    "        C, T, V, M = data_shape\n",
    "        self.register_buffer('A', A)\n",
    "\n",
    "        # data normalization\n",
    "        self.data_bn = nn.BatchNorm1d(C * V * M)\n",
    "\n",
    "        # st-gcn networks\n",
    "        self.st_gcn_networks = nn.ModuleList((\n",
    "            st_gcn_layer(C, 64, gcn_kernel_size, 1, A, drop_prob, residual=False),\n",
    "            st_gcn_layer(64, 64, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(64, 64, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(64, 64, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(64, 128, gcn_kernel_size, 2, A, drop_prob),\n",
    "            st_gcn_layer(128, 128, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(128, 128, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(128, 256, gcn_kernel_size, 2, A, drop_prob),\n",
    "            st_gcn_layer(256, 256, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(256, 256, gcn_kernel_size, 1, A, drop_prob),\n",
    "        ))\n",
    "\n",
    "        # edge importance weights\n",
    "        self.edge_importance = nn.ParameterList([nn.Parameter(torch.ones(A.shape)) for _ in self.st_gcn_networks])\n",
    "\n",
    "        # fcn\n",
    "        self.fcn = nn.Conv2d(256, num_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # data normalization\n",
    "        N, C, T, V, M = x.shape\n",
    "        x = x.permute(0, 4, 3, 1, 2).contiguous()\n",
    "        x = x.view(N, M * V * C, T)\n",
    "        x = self.data_bn(x)\n",
    "        x = x.view(N, M, V, C, T)\n",
    "        x = x.permute(0, 1, 3, 4, 2).contiguous()\n",
    "        x = x.view(N * M, C, T, V)\n",
    "\n",
    "        # forward\n",
    "        for gcn, importance in zip(self.st_gcn_networks, self.edge_importance):\n",
    "            x = gcn(x, self.A * importance)\n",
    "\n",
    "        # extract feature\n",
    "        _, c, t, v = x.shape\n",
    "        feature = x.view(N, M, c, t, v).permute(0, 2, 3, 4, 1)\n",
    "\n",
    "        # global pooling\n",
    "        x = F.avg_pool2d(x, x.shape[2:])\n",
    "        x = x.view(N, M, -1, 1, 1).mean(dim=1)\n",
    "\n",
    "        # prediction\n",
    "        x = self.fcn(x)\n",
    "        x = x.view(N, -1)\n",
    "\n",
    "        return x, feature\n",
    "\n",
    "\n",
    "class st_gcn_layer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, A, drop_prob=0, residual=True):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(kernel_size) == 2\n",
    "        assert kernel_size[0] % 2 == 1\n",
    "        padding = ((kernel_size[0] - 1) // 2, 0)\n",
    "\n",
    "        # spatial network\n",
    "        self.gcn = SpatialGraphConv(in_channels, out_channels, kernel_size[1]+1)\n",
    "\n",
    "        # temporal network\n",
    "        self.tcn = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_prob),\n",
    "            nn.Conv2d(out_channels, out_channels, (kernel_size[0],1), (stride,1), padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        # residual\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=(stride, 1)), nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        # output\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, A):\n",
    "\n",
    "        # residual\n",
    "        res = self.residual(x)\n",
    "\n",
    "        # spatial gcn\n",
    "        x = self.gcn(x, A)\n",
    "\n",
    "        # temporal 1d-cnn\n",
    "        x = self.tcn(x)\n",
    "\n",
    "        # output\n",
    "        x = self.relu(x + res)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpatialGraphConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, s_kernel_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # spatial class number (distance = 0 for class 0, distance = 1 for class 1, ...)\n",
    "        self.s_kernel_size = s_kernel_size\n",
    "\n",
    "        # weights of different spatial classes\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels * s_kernel_size, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, A):\n",
    "\n",
    "        # numbers in same class have same weight\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # divide into different classes\n",
    "        n, kc, t, v = x.shape\n",
    "        x = x.view(n, self.s_kernel_size, kc//self.s_kernel_size, t, v)\n",
    "\n",
    "        # spatial graph convolution\n",
    "        x = torch.einsum('nkctv,kvw->nctw', (x, A[:self.s_kernel_size])).contiguous()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "bXO3V41o8wz7"
   },
   "source": [
    "To make use of the graph structure of our input data, we define a Graph class that can be used on all datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "GsGUsmGo8wz8",
    "ExecuteTime": {
     "end_time": "2023-08-23T12:53:01.926258500Z",
     "start_time": "2023-08-23T12:53:01.913235500Z"
    }
   },
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self, max_hop=1, dilation=1):\n",
    "        self.max_hop = max_hop\n",
    "        self.dilation = dilation\n",
    "\n",
    "        # get edges\n",
    "        self.num_node, self.edge, self.center = self._get_edge()\n",
    "\n",
    "        # get adjacency matrix\n",
    "        self.hop_dis = self._get_hop_distance()\n",
    "\n",
    "        # normalization\n",
    "        self.A = self._get_adjacency()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.A\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_edge():\n",
    "        num_node = 17\n",
    "        neighbor_1base = [(1, 2), (1, 3), (2, 4), (3, 5), (6, 1), (6, 8),\n",
    "                          (7, 1), (7, 9), (8, 10), (9, 11), (12, 6), (12, 14),\n",
    "                          (13, 7), (13, 15), (14, 16), (15, 17)]\n",
    "        self_link = [(i, i) for i in range(num_node)]\n",
    "        neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_1base]\n",
    "        edge = self_link + neighbor_link\n",
    "        center = 0\n",
    "        return (num_node, edge, center)\n",
    "\n",
    "    def _get_hop_distance(self):\n",
    "        A = np.zeros((self.num_node, self.num_node))\n",
    "        for i, j in self.edge:\n",
    "            A[j, i] = 1\n",
    "            A[i, j] = 1\n",
    "        hop_dis = np.zeros((self.num_node, self.num_node)) + np.inf\n",
    "        transfer_mat = [np.linalg.matrix_power(A, d) for d in range(self.max_hop + 1)]\n",
    "        arrive_mat = (np.stack(transfer_mat) > 0)\n",
    "        for d in range(self.max_hop, -1, -1):\n",
    "            hop_dis[arrive_mat[d]] = d\n",
    "        return hop_dis\n",
    "\n",
    "    def _get_adjacency(self):\n",
    "        valid_hop = range(0, self.max_hop + 1, self.dilation)\n",
    "        adjacency = np.zeros((self.num_node, self.num_node))\n",
    "        for hop in valid_hop:\n",
    "            adjacency[self.hop_dis == hop] = 1\n",
    "        normalize_adjacency = self._normalize_digraph(adjacency)\n",
    "        A = np.zeros((len(valid_hop), self.num_node, self.num_node))\n",
    "        for i, hop in enumerate(valid_hop):\n",
    "            A[i][self.hop_dis == hop] = normalize_adjacency[self.hop_dis == hop]\n",
    "        return A\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_digraph(A):\n",
    "        Dl = np.sum(A, 0)\n",
    "        num_node = A.shape[0]\n",
    "        Dn = np.zeros((num_node, num_node))\n",
    "        for i in range(num_node):\n",
    "            if Dl[i] > 0:\n",
    "                Dn[i, i] = Dl[i]**(-1)\n",
    "        AD = np.dot(A, Dn)\n",
    "        return AD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8H9dDlgD8wz9",
    "ExecuteTime": {
     "end_time": "2023-08-23T12:53:01.946279500Z",
     "start_time": "2023-08-23T12:53:01.929768900Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Data_transform():\n",
    "    def __init__(self, data_transform=True):\n",
    "        self.data_transform = data_transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.data_transform:\n",
    "            C, T, V, M = x.shape\n",
    "            # print(f'x.shape: {x.shape}')\n",
    "            x_new = np.zeros((C*3, T, V, M))\n",
    "            x_new[:C,:,:,:] = x\n",
    "            for i in range(T-1):\n",
    "                x_new[C:(2*C),i,:,:] = x[:,i+1,:,:] - x[:,i,:,:]\n",
    "            for i in range(V):\n",
    "                x_new[(2*C):,:,i,:] = x[:,:,i,:] - x[:,:,1,:]\n",
    "            # print(f'x_new.shape: {x_new.shape}')\n",
    "            return x_new\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class Occlusion_part():\n",
    "    def __init__(self, occlusion_part=[]):\n",
    "        self.occlusion_part = occlusion_part\n",
    "\n",
    "        self.parts = dict()\n",
    "        self.parts[1] = np.array([7, 9, 11])                        # left arm\n",
    "        self.parts[2] = np.array([6, 8, 10])                        # right arm\n",
    "        self.parts[3] = np.array([10, 11])                          # two hands\n",
    "        self.parts[4] = np.array([12, 13, 14, 15, 16, 17])          # two legs\n",
    "        self.parts[5] = np.array([0, 1, 2, 3, 4, 5])                # head\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for part in self.occlusion_part:\n",
    "            x[:,:,self.parts[part],:] = 0\n",
    "        return x\n",
    "\n",
    "\n",
    "class Occlusion_time():\n",
    "    def __init__(self, occlusion_time=0):\n",
    "        self.occlusion_time = int(occlusion_time // 2)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if not self.occlusion_time == 0:\n",
    "            x[:,(45-self.occlusion_time):(45+self.occlusion_time),:,:] = 0\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2fvDhK2n8wz_",
    "ExecuteTime": {
     "end_time": "2023-08-23T12:53:01.985881700Z",
     "start_time": "2023-08-23T12:53:01.943280900Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(self, model_stream, module):\n",
    "        super(Mask, self).__init__()\n",
    "        self.model_stream = model_stream\n",
    "        self.module = module\n",
    "\n",
    "        for i in range(model_stream):\n",
    "            nn.init.ones_(self.module.mask_stream[i])\n",
    "\n",
    "    def forward(self, weight, feature):\n",
    "        result = []\n",
    "        for i in range(self.model_stream):\n",
    "            temp_result = self.CAM(weight[i], feature[i])\n",
    "            result.append(temp_result)\n",
    "\n",
    "        # Update mask matrices using inplace operations\n",
    "        for i in range(1, self.model_stream):\n",
    "            mask = torch.prod(torch.stack(result[:i]), dim=0)\n",
    "            mask = torch.cat([mask.unsqueeze(1)] * 4, dim=1)\n",
    "            self.module.mask_stream[i].data.copy_(mask.view(-1))\n",
    "\n",
    "    def CAM(self, weight, feature):\n",
    "        N, C = weight.shape\n",
    "        weight = weight.view(N, C, 1, 1, 1).expand_as(feature)\n",
    "        result = (weight * feature).sum(dim=1)\n",
    "        result = result.mean(dim=0)\n",
    "\n",
    "        T, V, M = result.shape\n",
    "        result = result.view(-1)\n",
    "\n",
    "        # Ensure that the result tensor size is consistent with the mask tensor\n",
    "        if result.size(0) > self.module.mask_stream[0].data.size(0):\n",
    "            result = result[:self.module.mask_stream[0].data.size(0)]\n",
    "\n",
    "        result = 1 - F.softmax(result, dim=0)\n",
    "        result = F.threshold(result, 0.1, 0)\n",
    "        result = result.view(T, V, M)\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "iVmFEf9MM9M6",
    "ExecuteTime": {
     "end_time": "2023-08-23T12:53:01.994676600Z",
     "start_time": "2023-08-23T12:53:01.960303400Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_weights(model_stream, module, y=None):\n",
    "    W = []\n",
    "    for i in range(model_stream):\n",
    "        temp_W = module.stgcn_stream[i].fcn.weight\n",
    "        if y is not None:\n",
    "            temp_W = temp_W[y,:]\n",
    "        W.append(temp_W.view(temp_W.shape[0], -1))\n",
    "    return W\n",
    "\n",
    "\n",
    "# Learning Rate Adjusting\n",
    "def adjust_lr(epoch, adjustLR, optimizer):\n",
    "    # LR decay\n",
    "    if epoch in adjustLR:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] /= 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "eWrHlemc8w0B",
    "ExecuteTime": {
     "end_time": "2023-08-23T12:53:01.995676100Z",
     "start_time": "2023-08-23T12:53:01.973858400Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch, epochs, model, train_loader, device, optimizer, loss_func, model_stream, mask_func):\n",
    "    acc, num_sample, total_loss = 0, 0, 0.0\n",
    "    model.train()\n",
    "    for num, (x, _, y, _) in enumerate(train_loader):\n",
    "        # Using GPU if possible\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Calculating Output\n",
    "        out, feature = model(x)\n",
    "\n",
    "        # update mask matrices\n",
    "        weight = get_weights(model_stream, model.module, y=y)\n",
    "        # TODO make the mask function work\n",
    "        # mask_func(weight, feature)\n",
    "\n",
    "        # Calculating Loss\n",
    "        loss = loss_func(out, y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Loss Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculating Accuracies\n",
    "        pred = out.max(1, keepdim=True)[1]\n",
    "        acc += pred.eq(y.view_as(pred)).sum().item()\n",
    "        num_sample += x.shape[0]\n",
    "\n",
    "        # Print Loss\n",
    "        print(f'Epoch: {epoch+1}/{epochs}, Batch: {num+1}/{len(train_loader)}, Loss: {loss:.4f}')\n",
    "\n",
    "    return total_loss / len(train_loader), acc / num_sample * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "fpn2l48o8w0B",
    "ExecuteTime": {
     "end_time": "2023-08-23T12:53:02.016967700Z",
     "start_time": "2023-08-23T12:53:01.991684900Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval(model, eval_loader, loss_func, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        acc, num_sample, total_loss = 0, 0, 0.0\n",
    "        for num, (x, _, y, _) in enumerate(eval_loader):\n",
    "\n",
    "            # Using GPU\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Calculating Output\n",
    "            out, _ = model(x)\n",
    "\n",
    "            # Calculating Loss\n",
    "            loss = loss_func(out, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculating Accuracies\n",
    "            pred = out.max(1, keepdim=True)[1]\n",
    "            acc += pred.eq(y.view_as(pred)).sum().item()\n",
    "            num_sample += x.shape[0]\n",
    "\n",
    "            # Print Progress\n",
    "            print(f'Batch: {num+1}/{len(eval_loader)}, Loss: {loss:.4f}')\n",
    "\n",
    "    return total_loss / len(eval_loader), acc / num_sample * 100"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ],
   "metadata": {
    "id": "Sd94He641Q_B",
    "ExecuteTime": {
     "end_time": "2023-08-23T12:53:02.034935Z",
     "start_time": "2023-08-23T12:53:02.005437100Z"
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Pfj1Zm128w0C",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e7cbb586-0e12-4a3c-dbb2-004bfe5d4c88",
    "ExecuteTime": {
     "end_time": "2023-08-23T13:00:41.774426500Z",
     "start_time": "2023-08-23T12:53:02.021978500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Batch: 1/25, Loss: 1.1755\n",
      "Epoch: 1/10, Batch: 2/25, Loss: 3.5798\n",
      "Epoch: 1/10, Batch: 3/25, Loss: 1.7865\n",
      "Epoch: 1/10, Batch: 4/25, Loss: 1.4233\n",
      "Epoch: 1/10, Batch: 5/25, Loss: 1.1322\n",
      "Epoch: 1/10, Batch: 6/25, Loss: 1.1960\n",
      "Epoch: 1/10, Batch: 7/25, Loss: 1.1857\n",
      "Epoch: 1/10, Batch: 8/25, Loss: 0.8997\n",
      "Epoch: 1/10, Batch: 9/25, Loss: 0.9151\n",
      "Epoch: 1/10, Batch: 10/25, Loss: 0.8281\n",
      "Epoch: 1/10, Batch: 11/25, Loss: 1.0780\n",
      "Epoch: 1/10, Batch: 12/25, Loss: 0.8097\n",
      "Epoch: 1/10, Batch: 13/25, Loss: 0.9365\n",
      "Epoch: 1/10, Batch: 14/25, Loss: 1.2365\n",
      "Epoch: 1/10, Batch: 15/25, Loss: 0.9036\n",
      "Epoch: 1/10, Batch: 16/25, Loss: 0.9804\n",
      "Epoch: 1/10, Batch: 17/25, Loss: 0.8470\n",
      "Epoch: 1/10, Batch: 18/25, Loss: 0.7991\n",
      "Epoch: 1/10, Batch: 19/25, Loss: 0.7744\n",
      "Epoch: 1/10, Batch: 20/25, Loss: 0.9577\n",
      "Epoch: 1/10, Batch: 21/25, Loss: 0.8883\n",
      "Epoch: 1/10, Batch: 22/25, Loss: 1.0188\n",
      "Epoch: 1/10, Batch: 23/25, Loss: 0.7364\n",
      "Epoch: 1/10, Batch: 24/25, Loss: 0.7484\n",
      "Epoch: 1/10, Batch: 25/25, Loss: 0.8428\n",
      "Batch: 1/6, Loss: 1.6671\n",
      "Batch: 2/6, Loss: 1.4390\n",
      "Batch: 3/6, Loss: 1.9183\n",
      "Batch: 4/6, Loss: 1.7892\n",
      "Batch: 5/6, Loss: 1.7734\n",
      "Batch: 6/6, Loss: 1.8449\n",
      "Epoch [1/10], Train Accuracy: 53.3125, Validation Accuracy: 36.9444             Train Loss: 1.1072, Validation Loss: 1.7387\n",
      "============================================================================\n",
      "Epoch: 2/10, Batch: 1/25, Loss: 0.6852\n",
      "Epoch: 2/10, Batch: 2/25, Loss: 0.5666\n",
      "Epoch: 2/10, Batch: 3/25, Loss: 0.5641\n",
      "Epoch: 2/10, Batch: 4/25, Loss: 0.6130\n",
      "Epoch: 2/10, Batch: 5/25, Loss: 0.7204\n",
      "Epoch: 2/10, Batch: 6/25, Loss: 0.6465\n",
      "Epoch: 2/10, Batch: 7/25, Loss: 0.4930\n",
      "Epoch: 2/10, Batch: 8/25, Loss: 0.6094\n",
      "Epoch: 2/10, Batch: 9/25, Loss: 0.4993\n",
      "Epoch: 2/10, Batch: 10/25, Loss: 0.3568\n",
      "Epoch: 2/10, Batch: 11/25, Loss: 0.6487\n",
      "Epoch: 2/10, Batch: 12/25, Loss: 0.6475\n",
      "Epoch: 2/10, Batch: 13/25, Loss: 0.4559\n",
      "Epoch: 2/10, Batch: 14/25, Loss: 0.4521\n",
      "Epoch: 2/10, Batch: 15/25, Loss: 0.5741\n",
      "Epoch: 2/10, Batch: 16/25, Loss: 0.4306\n",
      "Epoch: 2/10, Batch: 17/25, Loss: 0.4889\n",
      "Epoch: 2/10, Batch: 18/25, Loss: 0.4645\n",
      "Epoch: 2/10, Batch: 19/25, Loss: 0.3174\n",
      "Epoch: 2/10, Batch: 20/25, Loss: 0.5105\n",
      "Epoch: 2/10, Batch: 21/25, Loss: 0.3275\n",
      "Epoch: 2/10, Batch: 22/25, Loss: 0.7967\n",
      "Epoch: 2/10, Batch: 23/25, Loss: 0.7888\n",
      "Epoch: 2/10, Batch: 24/25, Loss: 0.6415\n",
      "Epoch: 2/10, Batch: 25/25, Loss: 0.5986\n",
      "Batch: 1/6, Loss: 0.7329\n",
      "Batch: 2/6, Loss: 0.5839\n",
      "Batch: 3/6, Loss: 0.6888\n",
      "Batch: 4/6, Loss: 0.7611\n",
      "Batch: 5/6, Loss: 0.6526\n",
      "Batch: 6/6, Loss: 0.8714\n",
      "Epoch [2/10], Train Accuracy: 79.9375, Validation Accuracy: 75.0000             Train Loss: 0.5559, Validation Loss: 0.7151\n",
      "============================================================================\n",
      "Epoch: 3/10, Batch: 1/25, Loss: 0.3961\n",
      "Epoch: 3/10, Batch: 2/25, Loss: 0.4288\n",
      "Epoch: 3/10, Batch: 3/25, Loss: 0.3672\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 56\u001B[0m\n\u001B[0;32m     52\u001B[0m early_stopper \u001B[38;5;241m=\u001B[39m EarlyStopper(patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, min_delta\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m)\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;66;03m# adjust_lr(epoch, [10,30], optimizer)\u001B[39;00m\n\u001B[1;32m---> 56\u001B[0m     t_loss, t_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_stream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask_func\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     57\u001B[0m     v_loss, v_acc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28meval\u001B[39m(model, eval_loader, criterion, DEVICE)\n\u001B[0;32m     59\u001B[0m     train_acc\u001B[38;5;241m.\u001B[39mappend(t_acc)\n",
      "Cell \u001B[1;32mIn[23], line 23\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(epoch, epochs, model, train_loader, device, optimizer, loss_func, model_stream, mask_func)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Loss Backward\u001B[39;00m\n\u001B[0;32m     22\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 23\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# Calculating Accuracies\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pedestrian-pose-recognition\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pedestrian-pose-recognition\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "data_transform = True\n",
    "\n",
    "# Data Loader Setting\n",
    "num_class = len(labels_to_learn)\n",
    "data_shape = (2, 90, 17, 1)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transform = transforms.Compose([\n",
    "    Data_transform(data_transform=data_transform),\n",
    "    Occlusion_part([]),\n",
    "    Occlusion_time(0),\n",
    "])\n",
    "\n",
    "train_dataset = NTU('../data/HRI_gestures/skeletons/', 'train', 0.8, data_shape, transform=transform, mode='cross-subject')\n",
    "eval_dataset = NTU('../data/HRI_gestures/skeletons/', 'eval', 0.8, data_shape, transform=transform, mode='cross-subject')\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                               batch_size=64, #num_workers=2*len([0]),\n",
    "                               pin_memory=True, shuffle=True, drop_last=True)\n",
    "eval_loader = DataLoader(eval_dataset,\n",
    "                              batch_size=64, #num_workers=2*len([0]),\n",
    "                              pin_memory=True, shuffle=False, drop_last=False)\n",
    "\n",
    "graph = Graph(max_hop=2)\n",
    "A = torch.tensor(graph.A, dtype=torch.float32, requires_grad=False).to(DEVICE)\n",
    "\n",
    "model_stream = 3\n",
    "\n",
    "if data_transform:\n",
    "  data_shape = (6, 90, 17, 1)\n",
    "\n",
    "# Model\n",
    "model = RA_GCN(data_shape, num_class, A, 0.5, [5,2], model_stream).to(DEVICE)\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer Setting\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001, nesterov=True)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Loss Function Setting\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "mask_func = Mask(model_stream, model.module)\n",
    "\n",
    "train_acc, train_loss = [], []\n",
    "val_acc, val_loss = [], []\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "early_stopper = EarlyStopper(patience=2, min_delta=0.3)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # adjust_lr(epoch, [10,30], optimizer)\n",
    "    t_loss, t_acc = train(epoch, num_epochs, model, train_loader, DEVICE, optimizer, criterion, model_stream, mask_func)\n",
    "    v_loss, v_acc = eval(model, eval_loader, criterion, DEVICE)\n",
    "\n",
    "    train_acc.append(t_acc)\n",
    "    val_acc.append(v_acc)\n",
    "    train_loss.append(t_loss)\n",
    "    val_loss.append(v_loss)\n",
    "\n",
    "    # Log training/validation loss for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_acc[epoch]:.4f}, Validation Accuracy: {val_acc[epoch]:.4f} \\\n",
    "            Train Loss: {train_loss[epoch]:.4f}, Validation Loss: {val_loss[epoch]:.4f}\")\n",
    "\n",
    "    if early_stopper.early_stop(val_loss[epoch]):\n",
    "        print('Stopping early to prevent overfitting and no more improvements!')\n",
    "        break\n",
    "\n",
    "    print(f'============================================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Save Model\n",
    "PATH = \"2_coord_Stop_StandingStill_FollowMe_CROSS_SUBJECT_10epoch.pth\"\n",
    "torch.save(model.state_dict(), PATH)"
   ],
   "metadata": {
    "id": "prjyv2_O8pwU",
    "ExecuteTime": {
     "end_time": "2023-08-23T13:00:41.783437600Z",
     "start_time": "2023-08-23T13:00:41.776425200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Accuracy %')\n",
    "tr_acc, = ax1.plot(range(1, num_epochs+1), train_acc, linestyle='-', color='r')\n",
    "eval_acc, = ax1.plot(range(1, num_epochs+1), val_acc, linestyle='-', color='g')\n",
    "ax1.set_yticks(np.arange(101, step=10))\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Loss')\n",
    "tr_loss, = ax2.plot(range(1, num_epochs+1), train_loss, linestyle='--', color='r')\n",
    "eval_loss, = ax2.plot(range(1, num_epochs+1), val_loss, linestyle='--', color='g')\n",
    "ax2.set_yticks([.0, .2, .4, .6, .8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0])\n",
    "ax2.grid(False)\n",
    "\n",
    "plt.legend([tr_acc, eval_acc, tr_loss, eval_loss], ['Training Acc', 'Eval Acc', 'Training Loss', 'Eval Loss'], loc='center right')\n",
    "fig.tight_layout()\n",
    "plt.savefig('Learning_cross-subject_10epoch.png')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-23T13:00:41.778427300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "eval_model = RA_GCN((6, 90, 17, 1), num_class, A, 0.5, [5,2], model_stream).to(DEVICE)\n",
    "eval_model = nn.DataParallel(eval_model)\n",
    "eval_model.load_state_dict(torch.load(PATH))\n",
    "eval_model.eval()"
   ],
   "metadata": {
    "id": "Qx10aW0imVVZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2ba7e73c-284c-4d8e-ac20-e4665a9c6381",
    "ExecuteTime": {
     "start_time": "2023-08-23T13:00:41.779426600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_data = np.load(\"../data/hri_keypoints_robert.npy\", allow_pickle=True)\n",
    "labels = np.load(\"../data/hri_labels_robert.npy\")\n",
    "num = 12\n",
    "\n",
    "# val_dataset = NTU('/content/drive/MyDrive/HRI_gestures/skeletons/', 'train', 0.8, (2, 90, 17, 1), transform=transform, mode='cross-person')\n",
    "\n",
    "for example in range(len(labels)):\n",
    "  if example == 12:\n",
    "    continue\n",
    "  x = np.zeros((2, 90, 17, 1))\n",
    "  for frame in range(len(input_data[example])):\n",
    "    for i in range(17):\n",
    "      x[0, frame, i, 0] = input_data[example][frame][i][0]\n",
    "      x[1, frame, i, 0] = input_data[example][frame][i][1]\n",
    "  x = transform(x)\n",
    "  x = torch.from_numpy(x).float()\n",
    "  x = x[None,:,:,:,:]\n",
    "  x = x.to(DEVICE)\n",
    "  out, _ = eval_model(x)\n",
    "  pred = out.max(1, keepdim=True)[1]\n",
    "  print(f'output: {pred}, output label: {labels_to_learn[pred]}')\n",
    "  print(f'label: {labels[example]}')"
   ],
   "metadata": {
    "id": "5ZPh99mXoStR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "10195076-fff8-4fea-80c9-157171656113",
    "ExecuteTime": {
     "start_time": "2023-08-23T13:00:41.780433100Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for num, (x, _, y, _) in enumerate(eval_loader):\n",
    "\n",
    "    # Using GPU\n",
    "    x = x.to(DEVICE)\n",
    "    y = y.to(DEVICE)\n",
    "\n",
    "    # Calculating Output\n",
    "    output, _ = model(x)\n",
    "    \n",
    "    output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "    y_pred.extend(output) # Save Prediction\n",
    "    \n",
    "    labels = y.data.cpu().numpy()\n",
    "    y_true.extend(labels) # Save Truth\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in labels_to_learn],\n",
    "                     columns = [i for i in labels_to_learn])\n",
    "plt.figure(figsize = (12,7))\n",
    "plt.xlabel('True Label')\n",
    "plt.ylabel('Predicted Label')\n",
    "s = sn.heatmap(df_cm, annot=True)\n",
    "s.set_xlabel('True Label', fontsize=20)\n",
    "s.set_ylabel('Predicted Label', fontsize=20)\n",
    "plt.savefig('cross-subject_10epoch.png')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-23T13:00:41.780433100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_classes = [0, 0, 0]\n",
    "num_training = [0, 0, 0]\n",
    "num_eval = [0, 0, 0]\n",
    "\n",
    "for _, _, label, _ in train_loader:\n",
    "    for idx in label:\n",
    "        num_classes[idx] += 1\n",
    "        num_training[idx] += 1\n",
    "for _, _, label, _ in eval_loader:\n",
    "    for idx in label:\n",
    "        num_classes[idx] += 1\n",
    "        num_eval[idx] += 1\n",
    "    \n",
    "print(num_classes)\n",
    "print(num_training)\n",
    "print(num_eval)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-23T13:00:41.781437700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-23T13:00:41.782438800Z"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
