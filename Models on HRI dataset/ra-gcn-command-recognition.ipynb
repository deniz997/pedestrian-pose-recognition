{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Richly Activated Graph Convolutional Network on HRI dataset\n",
    "\n",
    "We define a custom dataset that will be used to create training/test datasets. This will also help to create a DataLoader."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "action_class = {'A001': 'Stop', 'A002': 'Go Right', 'A003': 'Go Left', 'A004': 'Come Here', 'A005': 'Follow me',\n",
    "                'A006': 'Go Away', 'A007': 'Agree', 'A008': 'Disagree', 'A009': 'Go there', 'A010': 'Get Attention',\n",
    "                'A011': 'Be Quiet', 'A012': 'Dont Know', 'A013': 'Turn Around', 'A014': 'Take This',\n",
    "                'A015': 'Pick Up', 'A016': 'Standing Still', 'A017': 'Being Seated', 'A018': 'Walking Towards',\n",
    "                'A019': 'Walking Away', 'A020': 'Talking on Phone'}\n",
    "joint_dict = {'Nose': 0, 'LEye': 1, 'REye': 2, 'LEar': 3, 'REar': 4, 'LShoulder': 5, 'RShoulder': 6, 'LElbow': 7,\n",
    "              'RElbow': 8, 'LWrist': 9, 'RWrist': 10, 'LHip': 11, 'RHip': 12, 'LKnee': 13, 'RKnee': 14,\n",
    "              'LAnkle': 15, 'RAnkle': 16}\n",
    "\n",
    "maxC = 2\n",
    "maxT = 300\n",
    "maxV = 17\n",
    "maxM = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we will define the RA-GCN. This is based on the original RA-GCN implementation from:\n",
    "Song, Yi-Fan, Zhang Zhang, and Liang Wang. \"Richly activated graph convolutional network for action recognition with incomplete skeletons.\" 2019 IEEE International Conference on Image Processing (ICIP). IEEE, 2019."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class NTU(Dataset):\n",
    "    def __init__(self, path, data_shape=(2,300,17,1), transform=None):\n",
    "\n",
    "        self.path = path\n",
    "        self.maxC, self.maxT, self.maxV, self.maxM = data_shape\n",
    "        self.transform = transform\n",
    "        self.files = []\n",
    "\n",
    "        for dirpath, dirnames, filenames in os.walk(self.path):\n",
    "            self.files.append(filenames)\n",
    "\n",
    "        self.files = self.files[1]\n",
    "        self.files = np.array(self.files).flatten()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.files[idx].strip()\n",
    "        label = file_name.split('.')[0]\n",
    "        label = int(label.split('A')[1]) - 1\n",
    "\n",
    "        data = np.zeros((self.maxC, self.maxT, self.maxV, self.maxM))\n",
    "        location = np.zeros((2, self.maxT, self.maxV, self.maxM))\n",
    "        with open(self.path + file_name, 'r') as fr:\n",
    "            frame_num = int(fr.readline())\n",
    "            for frame in range(frame_num):\n",
    "                if frame >= self.maxT:\n",
    "                    break\n",
    "                person_num = int(fr.readline())\n",
    "                for person in range(person_num):\n",
    "                    fr.readline()\n",
    "                    joint_num = int(fr.readline())\n",
    "                    for joint in range(joint_num):\n",
    "                        v = fr.readline().split(' ')\n",
    "                        if joint < self.maxV and person < self.maxM:\n",
    "                            data[0,frame,joint,person] = float(v[0])\n",
    "                            data[1,frame,joint,person] = float(v[1])\n",
    "                            data[2,frame,joint,person] = float(v[2])\n",
    "                            location[0,frame,joint,person] = float(v[3])\n",
    "                            location[1,frame,joint,person] = float(v[4])\n",
    "\n",
    "        if frame_num <= self.maxT:\n",
    "            data = data[:,:self.maxT,:,:]\n",
    "        else:\n",
    "            s = frame_num // self.maxT\n",
    "            r = random.randint(0, frame_num - self.maxT * s)\n",
    "            new_data = np.zeros((self.maxC, self.maxT, self.maxV, self.maxM))\n",
    "            for i in range(self.maxT):\n",
    "                new_data[:,i,:,:] = data[:,r+s*i,:,:]\n",
    "            data = new_data\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        data = torch.from_numpy(data).float()\n",
    "        location = torch.from_numpy(location).float()\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "        return data, location, label, file_name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(self, model_stream, module):\n",
    "        super(Mask, self).__init__()\n",
    "        self.model_stream = model_stream\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, weight, feature):\n",
    "        result = []\n",
    "        for i in range(self.model_stream):\n",
    "            temp_result = self.CAM(weight[i], feature[i])\n",
    "            result.append(temp_result)\n",
    "        for i in range(1, self.model_stream):\n",
    "            for j in range(i):\n",
    "                if j == 0:\n",
    "                    mask = result[j]\n",
    "                else:\n",
    "                    mask *= result[j]\n",
    "            mask = torch.cat([mask.unsqueeze(1)] * 4, dim=1)\n",
    "            self.module.mask_stream[i].data = mask.view(-1).detach()\n",
    "\n",
    "    def CAM(self, weight, feature):\n",
    "        N, C = weight.shape\n",
    "        weight = weight.view(N, C, 1, 1, 1).expand_as(feature)\n",
    "        result = (weight * feature).sum(dim=1)\n",
    "        result = result.mean(dim=0)\n",
    "\n",
    "        T, V, M = result.shape\n",
    "        result = result.view(-1)\n",
    "        result = 1 - F.softmax(result, dim=0)\n",
    "        result = F.threshold(result, 0.1, 0)\n",
    "        result = result.view(T, V, M)\n",
    "        return result\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class RA_GCN(nn.Module):\n",
    "    def __init__(self, data_shape, num_class, A, drop_prob, gcn_kernel_size, model_stream):\n",
    "        super().__init__()\n",
    "\n",
    "        C, T, V, M = data_shape\n",
    "        self.register_buffer('A', A)\n",
    "\n",
    "        # baseline\n",
    "        self.stgcn_stream = nn.ModuleList((\n",
    "            ST_GCN(data_shape, num_class, A, drop_prob, gcn_kernel_size)\n",
    "            for _ in range(model_stream)\n",
    "        ))\n",
    "\n",
    "        # mask\n",
    "        self.mask_stream = nn.ParameterList([\n",
    "            nn.Parameter(torch.ones(T * V * M))\n",
    "            for _ in range(model_stream)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        # multi stream\n",
    "        out = 0\n",
    "        feature = []\n",
    "        for stgcn, mask in zip(self.stgcn_stream, self.mask_stream):\n",
    "            x = inp\n",
    "\n",
    "            # mask\n",
    "            N, C, T, V, M = x.shape\n",
    "            x = x.view(N, C, -1)\n",
    "            x = x * mask[None,None,:]\n",
    "            x = x.view(N, C, T, V, M)\n",
    "\n",
    "            # baseline\n",
    "            temp_out, temp_feature = stgcn(x)\n",
    "\n",
    "            # output\n",
    "            out += temp_out\n",
    "            feature.append(temp_feature)\n",
    "        return out, feature\n",
    "\n",
    "\n",
    "class ST_GCN(nn.Module):\n",
    "    def __init__(self, data_shape, num_class, A, drop_prob, gcn_kernel_size):\n",
    "        super().__init__()\n",
    "\n",
    "        C, T, V, M = data_shape\n",
    "        self.register_buffer('A', A)\n",
    "\n",
    "        # data normalization\n",
    "        self.data_bn = nn.BatchNorm1d(C * V * M)\n",
    "\n",
    "        # st-gcn networks\n",
    "        self.st_gcn_networks = nn.ModuleList((\n",
    "            st_gcn_layer(C, 64, gcn_kernel_size, 1, A, drop_prob, residual=False),\n",
    "            st_gcn_layer(64, 64, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(64, 64, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(64, 64, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(64, 128, gcn_kernel_size, 2, A, drop_prob),\n",
    "            st_gcn_layer(128, 128, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(128, 128, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(128, 256, gcn_kernel_size, 2, A, drop_prob),\n",
    "            st_gcn_layer(256, 256, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(256, 256, gcn_kernel_size, 1, A, drop_prob),\n",
    "        ))\n",
    "\n",
    "        # edge importance weights\n",
    "        self.edge_importance = nn.ParameterList([nn.Parameter(torch.ones(A.shape)) for _ in self.st_gcn_networks])\n",
    "\n",
    "        # fcn\n",
    "        self.fcn = nn.Conv2d(256, num_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # data normalization\n",
    "        N, C, T, V, M = x.shape\n",
    "        x = x.permute(0, 4, 3, 1, 2).contiguous()\n",
    "        x = x.view(N, M * V * C, T)\n",
    "        x = self.data_bn(x)\n",
    "        x = x.view(N, M, V, C, T)\n",
    "        x = x.permute(0, 1, 3, 4, 2).contiguous()\n",
    "        x = x.view(N * M, C, T, V)\n",
    "\n",
    "        # forward\n",
    "        for gcn, importance in zip(self.st_gcn_networks, self.edge_importance):\n",
    "            x = gcn(x, self.A * importance)\n",
    "\n",
    "        # extract feature\n",
    "        _, c, t, v = x.shape\n",
    "        feature = x.view(N, M, c, t, v).permute(0, 2, 3, 4, 1)\n",
    "\n",
    "        # global pooling\n",
    "        x = F.avg_pool2d(x, x.shape[2:])\n",
    "        x = x.view(N, M, -1, 1, 1).mean(dim=1)\n",
    "\n",
    "        # prediction\n",
    "        x = self.fcn(x)\n",
    "        x = x.view(N, -1)\n",
    "\n",
    "        return x, feature\n",
    "\n",
    "\n",
    "class st_gcn_layer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, A, drop_prob=0, residual=True):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(kernel_size) == 2\n",
    "        assert kernel_size[0] % 2 == 1\n",
    "        padding = ((kernel_size[0] - 1) // 2, 0)\n",
    "\n",
    "        # spatial network\n",
    "        self.gcn = SpatialGraphConv(in_channels, out_channels, kernel_size[1]+1)\n",
    "\n",
    "        # temporal network\n",
    "        self.tcn = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_prob),\n",
    "            nn.Conv2d(out_channels, out_channels, (kernel_size[0],1), (stride,1), padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        # residual\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=(stride, 1)), nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        # output\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, A):\n",
    "\n",
    "        # residual\n",
    "        res = self.residual(x)\n",
    "\n",
    "        # spatial gcn\n",
    "        x = self.gcn(x, A)\n",
    "\n",
    "        # temporal 1d-cnn\n",
    "        x = self.tcn(x)\n",
    "\n",
    "        # output\n",
    "        x = self.relu(x + res)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpatialGraphConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, s_kernel_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # spatial class number (distance = 0 for class 0, distance = 1 for class 1, ...)\n",
    "        self.s_kernel_size = s_kernel_size\n",
    "\n",
    "        # weights of different spatial classes\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels * s_kernel_size, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, A):\n",
    "\n",
    "        # numbers in same class have same weight\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # divide into different classes\n",
    "        n, kc, t, v = x.shape\n",
    "        x = x.view(n, self.s_kernel_size, kc//self.s_kernel_size, t, v)\n",
    "\n",
    "        # spatial graph convolution\n",
    "        x = torch.einsum('nkctv,kvw->nctw', (x, A[:self.s_kernel_size])).contiguous()\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make use of the graph structure of our input data, we define a Graph class that can be used on all datapoints."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self, max_hop=1, dilation=1):\n",
    "        self.max_hop = max_hop\n",
    "        self.dilation = dilation\n",
    "\n",
    "        # get edges\n",
    "        self.num_node, self.edge, self.center = self._get_edge()\n",
    "\n",
    "        # get adjacency matrix\n",
    "        self.hop_dis = self._get_hop_distance()\n",
    "\n",
    "        # normalization\n",
    "        self.A = self._get_adjacency()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.A\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_edge():\n",
    "        num_node = 17\n",
    "        neighbor_1base = [(1, 2), (1, 3), (2, 4), (3, 5), (6, 1), (6, 8),\n",
    "                          (7, 1), (7, 9), (8, 10), (9, 11), (12, 6), (12, 14),\n",
    "                          (13, 7), (13, 15), (14, 16), (15, 17)]\n",
    "        self_link = [(i, i) for i in range(num_node)]\n",
    "        neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_1base]\n",
    "        edge = self_link + neighbor_link\n",
    "        center = 0\n",
    "        return num_node, edge, center\n",
    "\n",
    "    def _get_hop_distance(self):\n",
    "        A = np.zeros((self.num_node, self.num_node))\n",
    "        for i, j in self.edge:\n",
    "            A[j, i] = 1\n",
    "            A[i, j] = 1\n",
    "        hop_dis = np.zeros((self.num_node, self.num_node)) + np.inf\n",
    "        transfer_mat = [np.linalg.matrix_power(A, d) for d in range(self.max_hop + 1)]\n",
    "        arrive_mat = (np.stack(transfer_mat) > 0)\n",
    "        for d in range(self.max_hop, -1, -1):\n",
    "            hop_dis[arrive_mat[d]] = d\n",
    "        return hop_dis\n",
    "\n",
    "    def _get_adjacency(self):\n",
    "        valid_hop = range(0, self.max_hop + 1, self.dilation)\n",
    "        adjacency = np.zeros((self.num_node, self.num_node))\n",
    "        for hop in valid_hop:\n",
    "            adjacency[self.hop_dis == hop] = 1\n",
    "        normalize_adjacency = self._normalize_digraph(adjacency)\n",
    "        A = np.zeros((len(valid_hop), self.num_node, self.num_node))\n",
    "        for i, hop in enumerate(valid_hop):\n",
    "            A[i][self.hop_dis == hop] = normalize_adjacency[self.hop_dis == hop]\n",
    "        return A\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_digraph(A):\n",
    "        Dl = np.sum(A, 0)\n",
    "        num_node = A.shape[0]\n",
    "        Dn = np.zeros((num_node, num_node))\n",
    "        for i in range(num_node):\n",
    "            if Dl[i] > 0:\n",
    "                Dn[i, i] = Dl[i]**(-1)\n",
    "        AD = np.dot(A, Dn)\n",
    "        return AD\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "class Data_transform():\n",
    "    def __init__(self, data_transform=True):\n",
    "        self.data_transform = data_transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.data_transform:\n",
    "            C, T, V, M = x.shape\n",
    "            x_new = np.zeros((C*3, T, V, M))\n",
    "            x_new[:C,:,:,:] = x\n",
    "            for i in range(T-1):\n",
    "                x_new[C:(2*C),i,:,:] = x[:,i+1,:,:] - x[:,i,:,:]\n",
    "            for i in range(V):\n",
    "                x_new[(2*C):,:,i,:] = x[:,:,i,:] - x[:,:,1,:]\n",
    "            return x_new\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class Occlusion_part():\n",
    "    def __init__(self, occlusion_part=[]):\n",
    "        self.occlusion_part = occlusion_part\n",
    "\n",
    "        self.parts = dict()\n",
    "        self.parts[1] = np.array([7, 9, 11])              # left arm\n",
    "        self.parts[2] = np.array([6, 8, 10])           # right arm\n",
    "        self.parts[3] = np.array([10, 11])                  # two hands\n",
    "        self.parts[4] = np.array([12, 13, 14, 15, 16, 17])  # two legs\n",
    "        self.parts[5] = np.array([0, 1, 2, 3, 4, 5])                  # head\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for part in self.occlusion_part:\n",
    "            x[:,:,self.parts[part],:] = 0\n",
    "        return x\n",
    "\n",
    "\n",
    "class Occlusion_time():\n",
    "    def __init__(self, occlusion_time=0):\n",
    "        self.occlusion_time = int(occlusion_time // 2)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if not self.occlusion_time == 0:\n",
    "            x[:,(50-self.occlusion_time):(50+self.occlusion_time),:,:] = 0\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(self, model_stream, module):\n",
    "        super(Mask, self).__init__()\n",
    "        self.model_stream = model_stream\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, weight, feature):\n",
    "        result = []\n",
    "        for i in range(self.model_stream):\n",
    "            temp_result = self.CAM(weight[i], feature[i])\n",
    "            result.append(temp_result)\n",
    "        for i in range(1, self.model_stream):\n",
    "            for j in range(i):\n",
    "                if j == 0:\n",
    "                    mask = result[j]\n",
    "                else:\n",
    "                    mask *= result[j]\n",
    "            mask = torch.cat([mask.unsqueeze(1)] * 4, dim=1)\n",
    "            self.module.mask_stream[i].data = mask.view(-1).detach()\n",
    "\n",
    "    def CAM(self, weight, feature):\n",
    "        N, C = weight.shape\n",
    "        weight = weight.view(N, C, 1, 1, 1).expand_as(feature)\n",
    "        result = (weight * feature).sum(dim=1)\n",
    "        result = result.mean(dim=0)\n",
    "\n",
    "        T, V, M = result.shape\n",
    "        result = result.view(-1)\n",
    "        result = 1 - F.softmax(result, dim=0)\n",
    "        result = F.threshold(result, 0.1, 0)\n",
    "        result = result.view(T, V, M)\n",
    "        return result\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def training_loop(model, train_loader):\n",
    "    # Train Loop\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    print(f'Starting Training Loop!')\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        targets = targets.long()\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate Loss\n",
    "        epoch_loss += loss.item() * images.size(0)\n",
    "\n",
    "    return epoch_loss / len(train_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def val_loop(model, test_loader):\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    tmp_loss = 0.0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            outputs = model(images)\n",
    "\n",
    "            # get the predicted pixel by using the maximum\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            targets = targets.long()\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            tmp_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # add predictions and true labels as numpy arrays to list\n",
    "            predictions.extend(predicted.view(-1).cpu().numpy())\n",
    "            true_labels.extend(targets.view(-1).cpu().numpy())\n",
    "\n",
    "    return tmp_loss / len(eval_dataset), true_labels, predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop!\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Data Loader Setting\n",
    "num_class = 12\n",
    "data_shape = (2, 300, 25, 1)\n",
    "transform = transforms.Compose([\n",
    "    Data_transform(True),\n",
    "    Occlusion_part([]),\n",
    "    Occlusion_time(0),\n",
    "])\n",
    "\n",
    "train_dataset = NTU('../data/HRI_gestures/skeletons/', data_shape, transform=transform)\n",
    "eval_dataset = NTU('../data/HRI_gestures/skeletons/', data_shape, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                               batch_size=16, num_workers=2*len([0]),\n",
    "                               pin_memory=True, shuffle=True, drop_last=True)\n",
    "eval_loader = DataLoader(eval_dataset,\n",
    "                              batch_size=16, num_workers=2*len([0]),\n",
    "                              pin_memory=True, shuffle=False, drop_last=False)\n",
    "\n",
    "graph = Graph(max_hop=2)\n",
    "A = torch.tensor(graph.A, dtype=torch.float32, requires_grad=False).to(torch.device('cpu'))\n",
    "\n",
    "# Model\n",
    "model = RA_GCN(data_shape, num_class, A, 0.5, [5,2], 3).to(torch.device('cpu'))\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "\n",
    "# Optimizer Setting\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001, nesterov=True)\n",
    "\n",
    "# Loss Function Setting\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Mask Function Setting\n",
    "mask_func = Mask(3, model.module)\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    t_loss = training_loop(model, train_loader)\n",
    "    v_loss, truth, pred = val_loop(model, eval_loader)\n",
    "\n",
    "    train_loss.append(t_loss)\n",
    "    val_loss.append(v_loss)\n",
    "\n",
    "    # Log training/validation loss for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss[epoch]:.4f}, Validation Loss: {val_loss[epoch]:.4f}\")\n",
    "\n",
    "    print(f'============================================================================')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
