{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "LOVdxGbx8wzs"
   },
   "source": [
    "# Richly Activated Graph Convolutional Network on HRI dataset\n",
    "\n",
    "We define a custom dataset that will be used to create training/test datasets. This will also help to create a DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "TC06Hli28wzx"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "action_class = {'A001': 'Stop', 'A002': 'Go Right', 'A003': 'Go Left', 'A004': 'Come Here', 'A005': 'Follow me',\n",
    "                'A006': 'Go Away', 'A007': 'Agree', 'A008': 'Disagree', 'A009': 'Go there', 'A010': 'Get Attention',\n",
    "                'A011': 'Be Quiet', 'A012': 'Dont Know', 'A013': 'Turn Around', 'A014': 'Take This',\n",
    "                'A015': 'Pick Up', 'A016': 'Standing Still', 'A017': 'Being Seated', 'A018': 'Walking Towards',\n",
    "                'A019': 'Walking Away', 'A020': 'Talking on Phone'}\n",
    "lab_to_num = {'Stop': 0, 'Go Right': 1, 'Go Left': 2, 'Come Here': 3, 'Follow me': 4, 'Go Away': 5, 'Agree': 6, 'Disagree': 7,\n",
    "              'Go there': 8, 'Get Attention': 9, 'Be Quiet': 10, 'Dont Know': 11, 'Turn Around': 12, 'Take This': 13,\n",
    "              'Pick Up': 14, 'Standing Still': 15, 'Being Seated': 16, 'Walking Towards': 17, 'Walking Away': 18,\n",
    "              'Talking on Phone': 19}\n",
    "joint_dict = {'Nose': 0, 'LEye': 1, 'REye': 2, 'LEar': 3, 'REar': 4, 'LShoulder': 5, 'RShoulder': 6, 'LElbow': 7,\n",
    "              'RElbow': 8, 'LWrist': 9, 'RWrist': 10, 'LHip': 11, 'RHip': 12, 'LKnee': 13, 'RKnee': 14,\n",
    "              'LAnkle': 15, 'RAnkle': 16}\n",
    "\n",
    "maxC = 2\n",
    "maxT = 300\n",
    "maxV = 17\n",
    "maxM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35r5TAJm9JCC",
    "outputId": "c5f59f1d-7be7-46fe-8ea1-2eb968c134ed"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "F88uKzCx8wzz"
   },
   "source": [
    "Next we will define the RA-GCN. This is based on the original RA-GCN implementation from:\n",
    "Song, Yi-Fan, Zhang Zhang, and Liang Wang. \"Richly activated graph convolutional network for action recognition with incomplete skeletons.\" 2019 IEEE International Conference on Image Processing (ICIP). IEEE, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "RKMc4IZA8wz2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class NTU(Dataset):\n",
    "    def __init__(self, path, train, train_split, data_shape=(3,300,17,1), transform=None):\n",
    "\n",
    "        self.train = train\n",
    "        self.path = path\n",
    "        self.maxC, self.maxT, self.maxV, self.maxM = data_shape\n",
    "        self.transform = transform\n",
    "        self.files = []\n",
    "\n",
    "        for dirpath, dirnames, filenames in os.walk(self.path):\n",
    "            self.files.append(filenames)\n",
    "\n",
    "        self.files = self.files[1]\n",
    "        self.files = np.array(self.files).flatten()\n",
    "        if self.train == 'train':\n",
    "          self.files = self.files[:int(len(self.files) * train_split)]\n",
    "        elif self.train == 'eval':\n",
    "          self.files = self.files[int(len(self.files) * train_split):]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.files[idx].strip()\n",
    "\n",
    "        data = np.zeros((self.maxC, self.maxT, self.maxV, self.maxM))\n",
    "        location = np.zeros((2, self.maxT, self.maxV, self.maxM))\n",
    "        with open(self.path + file_name[4:8] + '/' + file_name, 'r') as fr:\n",
    "            frame_num = int(fr.readline())\n",
    "            label = lab_to_num[action_class[file_name[:4]]]\n",
    "            for frame in range(frame_num):\n",
    "                if frame >= self.maxT:\n",
    "                    break\n",
    "                joint_num = int(fr.readline())\n",
    "                for joint in range(joint_num):\n",
    "                    v = fr.readline().split(' ')\n",
    "                    if joint < self.maxV:\n",
    "                        data[0,frame,joint,0] = float(v[1])\n",
    "                        data[1,frame,joint,0] = float(v[2])\n",
    "                        data[2,frame,joint,0] = float(v[3])\n",
    "                        location[0,frame,joint,0] = float(v[4])\n",
    "                        location[1,frame,joint,0] = float(v[5])\n",
    "\n",
    "        if frame_num <= self.maxT:\n",
    "            data = data[:,:self.maxT,:,:]\n",
    "        else:\n",
    "            s = frame_num // self.maxT\n",
    "            r = random.randint(0, frame_num - self.maxT * s)\n",
    "            new_data = np.zeros((self.maxC, self.maxT, self.maxV, self.maxM))\n",
    "            for i in range(self.maxT):\n",
    "                new_data[:,i,:,:] = data[:,r+s*i,:,:]\n",
    "            data = new_data\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        data = torch.from_numpy(data).float()\n",
    "        location = torch.from_numpy(location).float()\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "        # print(f'data_shape: {data.shape}\\nlabel: {label}')\n",
    "        return data, location, label, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "7gOuwi5p8wz6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class RA_GCN(nn.Module):\n",
    "    def __init__(self, data_shape, num_class, A, drop_prob, gcn_kernel_size, model_stream):\n",
    "        super().__init__()\n",
    "\n",
    "        C, T, V, M = data_shape\n",
    "        self.register_buffer('A', A)\n",
    "\n",
    "        # baseline\n",
    "        self.stgcn_stream = nn.ModuleList((\n",
    "            ST_GCN(data_shape, num_class, A, drop_prob, gcn_kernel_size)\n",
    "            for _ in range(model_stream)\n",
    "        ))\n",
    "\n",
    "        # mask\n",
    "        self.mask_stream = nn.ParameterList([\n",
    "            nn.Parameter(torch.ones(T * V))\n",
    "            for _ in range(model_stream)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        # multi stream\n",
    "        out = 0\n",
    "        feature = []\n",
    "        for stgcn, mask in zip(self.stgcn_stream, self.mask_stream):\n",
    "            x = inp\n",
    "\n",
    "            # mask\n",
    "            N, C, T, V, M = x.shape\n",
    "            x = x.view(N, C, -1)\n",
    "            x = x * mask[None,None,:]\n",
    "            x = x.view(N, C, T, V, M)\n",
    "\n",
    "            # baseline\n",
    "            temp_out, temp_feature = stgcn(x)\n",
    "\n",
    "            # output\n",
    "            out += temp_out\n",
    "            feature.append(temp_feature)\n",
    "        print(f'RA-GCN Layer, out shape: {out.shape}, feature shape: {feature.shape}')\n",
    "        return out, feature\n",
    "\n",
    "\n",
    "class ST_GCN(nn.Module):\n",
    "    def __init__(self, data_shape, num_class, A, drop_prob, gcn_kernel_size):\n",
    "        super().__init__()\n",
    "\n",
    "        C, T, V, M = data_shape\n",
    "        self.register_buffer('A', A)\n",
    "\n",
    "        # data normalization\n",
    "        self.data_bn = nn.BatchNorm1d(C * V * M)\n",
    "\n",
    "        # st-gcn networks\n",
    "        self.st_gcn_networks = nn.ModuleList((\n",
    "            st_gcn_layer(C, 64, gcn_kernel_size, 1, A, drop_prob, residual=False),\n",
    "            st_gcn_layer(64, 64, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(64, 64, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(64, 64, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(64, 128, gcn_kernel_size, 2, A, drop_prob),\n",
    "            st_gcn_layer(128, 128, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(128, 128, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(128, 256, gcn_kernel_size, 2, A, drop_prob),\n",
    "            st_gcn_layer(256, 256, gcn_kernel_size, 1, A, drop_prob),\n",
    "            st_gcn_layer(256, 256, gcn_kernel_size, 1, A, drop_prob),\n",
    "        ))\n",
    "\n",
    "        # edge importance weights\n",
    "        self.edge_importance = nn.ParameterList([nn.Parameter(torch.ones(A.shape)) for _ in self.st_gcn_networks])\n",
    "\n",
    "        # fcn\n",
    "        self.fcn = nn.Conv2d(256, num_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # data normalization\n",
    "        N, C, T, V, M = x.shape\n",
    "        x = x.permute(0, 4, 3, 1, 2).contiguous()\n",
    "        x = x.view(N, M * V * C, T)\n",
    "        x = self.data_bn(x)\n",
    "        x = x.view(N, M, V, C, T)\n",
    "        x = x.permute(0, 1, 3, 4, 2).contiguous()\n",
    "        x = x.view(N * M, C, T, V)\n",
    "\n",
    "        # forward\n",
    "        for gcn, importance in zip(self.st_gcn_networks, self.edge_importance):\n",
    "            x = gcn(x, self.A * importance)\n",
    "\n",
    "        # extract feature\n",
    "        _, c, t, v = x.shape\n",
    "        feature = x.view(N, M, c, t, v).permute(0, 2, 3, 4, 1)\n",
    "\n",
    "        # global pooling\n",
    "        x = F.avg_pool2d(x, x.shape[2:])\n",
    "        x = x.view(N, M, -1, 1, 1).mean(dim=1)\n",
    "\n",
    "        # prediction\n",
    "        x = self.fcn(x)\n",
    "        x = x.view(N, -1)\n",
    "\n",
    "        print(f'ST-GCN layer, x shape: {x.shape}, feature shape: {feature.shape}')\n",
    "\n",
    "        return x, feature\n",
    "\n",
    "\n",
    "class st_gcn_layer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, A, drop_prob=0, residual=True):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(kernel_size) == 2\n",
    "        assert kernel_size[0] % 2 == 1\n",
    "        padding = ((kernel_size[0] - 1) // 2, 0)\n",
    "\n",
    "        # spatial network\n",
    "        self.gcn = SpatialGraphConv(in_channels, out_channels, kernel_size[1]+1)\n",
    "\n",
    "        # temporal network\n",
    "        self.tcn = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_prob),\n",
    "            nn.Conv2d(out_channels, out_channels, (kernel_size[0],1), (stride,1), padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        # residual\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=(stride, 1)), nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        # output\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, A):\n",
    "\n",
    "        # residual\n",
    "        res = self.residual(x)\n",
    "\n",
    "        # spatial gcn\n",
    "        x = self.gcn(x, A)\n",
    "\n",
    "        # temporal 1d-cnn\n",
    "        x = self.tcn(x)\n",
    "\n",
    "        # output\n",
    "        x = self.relu(x + res)\n",
    "        print(f'st-gcn layer, x shape: {x.shape}')\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpatialGraphConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, s_kernel_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # spatial class number (distance = 0 for class 0, distance = 1 for class 1, ...)\n",
    "        self.s_kernel_size = s_kernel_size\n",
    "\n",
    "        # weights of different spatial classes\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels * s_kernel_size, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, A):\n",
    "\n",
    "        # numbers in same class have same weight\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # divide into different classes\n",
    "        n, kc, t, v = x.shape\n",
    "        x = x.view(n, self.s_kernel_size, kc//self.s_kernel_size, t, v)\n",
    "\n",
    "        # spatial graph convolution\n",
    "        x = torch.einsum('nkctv,kvw->nctw', (x, A[:self.s_kernel_size])).contiguous()\n",
    "        print(f'spatial gc layer, x shape: {x.shape}')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "bXO3V41o8wz7"
   },
   "source": [
    "To make use of the graph structure of our input data, we define a Graph class that can be used on all datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "GsGUsmGo8wz8"
   },
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self, max_hop=1, dilation=1):\n",
    "        self.max_hop = max_hop\n",
    "        self.dilation = dilation\n",
    "\n",
    "        # get edges\n",
    "        self.num_node, self.edge, self.center = self._get_edge()\n",
    "\n",
    "        # get adjacency matrix\n",
    "        self.hop_dis = self._get_hop_distance()\n",
    "\n",
    "        # normalization\n",
    "        self.A = self._get_adjacency()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.A\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_edge():\n",
    "        num_node = 17\n",
    "        neighbor_1base = [(1, 2), (1, 3), (2, 4), (3, 5), (6, 1), (6, 8),\n",
    "                          (7, 1), (7, 9), (8, 10), (9, 11), (12, 6), (12, 14),\n",
    "                          (13, 7), (13, 15), (14, 16), (15, 17)]\n",
    "        self_link = [(i, i) for i in range(num_node)]\n",
    "        neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_1base]\n",
    "        edge = self_link + neighbor_link\n",
    "        center = 0\n",
    "        return (num_node, edge, center)\n",
    "\n",
    "    def _get_hop_distance(self):\n",
    "        A = np.zeros((self.num_node, self.num_node))\n",
    "        for i, j in self.edge:\n",
    "            A[j, i] = 1\n",
    "            A[i, j] = 1\n",
    "        hop_dis = np.zeros((self.num_node, self.num_node)) + np.inf\n",
    "        transfer_mat = [np.linalg.matrix_power(A, d) for d in range(self.max_hop + 1)]\n",
    "        arrive_mat = (np.stack(transfer_mat) > 0)\n",
    "        for d in range(self.max_hop, -1, -1):\n",
    "            hop_dis[arrive_mat[d]] = d\n",
    "        return hop_dis\n",
    "\n",
    "    def _get_adjacency(self):\n",
    "        valid_hop = range(0, self.max_hop + 1, self.dilation)\n",
    "        adjacency = np.zeros((self.num_node, self.num_node))\n",
    "        for hop in valid_hop:\n",
    "            adjacency[self.hop_dis == hop] = 1\n",
    "        normalize_adjacency = self._normalize_digraph(adjacency)\n",
    "        A = np.zeros((len(valid_hop), self.num_node, self.num_node))\n",
    "        for i, hop in enumerate(valid_hop):\n",
    "            A[i][self.hop_dis == hop] = normalize_adjacency[self.hop_dis == hop]\n",
    "        return A\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_digraph(A):\n",
    "        Dl = np.sum(A, 0)\n",
    "        num_node = A.shape[0]\n",
    "        Dn = np.zeros((num_node, num_node))\n",
    "        for i in range(num_node):\n",
    "            if Dl[i] > 0:\n",
    "                Dn[i, i] = Dl[i]**(-1)\n",
    "        AD = np.dot(A, Dn)\n",
    "        return AD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "8H9dDlgD8wz9"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Data_transform():\n",
    "    def __init__(self, data_transform=True):\n",
    "        self.data_transform = data_transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.data_transform:\n",
    "            C, T, V, M = x.shape\n",
    "            x_new = np.zeros((C*3, T, V, M))\n",
    "            x_new[:C,:,:,:] = x\n",
    "            for i in range(T-1):\n",
    "                x_new[C:(2*C),i,:,:] = x[:,i+1,:,:] - x[:,i,:,:]\n",
    "            for i in range(V):\n",
    "                x_new[(2*C):,:,i,:] = x[:,:,i,:] - x[:,:,1,:]\n",
    "            return x_new\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class Occlusion_part():\n",
    "    def __init__(self, occlusion_part=[]):\n",
    "        self.occlusion_part = occlusion_part\n",
    "\n",
    "        self.parts = dict()\n",
    "        self.parts[1] = np.array([7, 9, 11])              # left arm\n",
    "        self.parts[2] = np.array([6, 8, 10])           # right arm\n",
    "        self.parts[3] = np.array([10, 11])                  # two hands\n",
    "        self.parts[4] = np.array([12, 13, 14, 15, 16, 17])  # two legs\n",
    "        self.parts[5] = np.array([0, 1, 2, 3, 4, 5])                  # head\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for part in self.occlusion_part:\n",
    "            x[:,:,self.parts[part],:] = 0\n",
    "        return x\n",
    "\n",
    "\n",
    "class Occlusion_time():\n",
    "    def __init__(self, occlusion_time=0):\n",
    "        self.occlusion_time = int(occlusion_time // 2)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if not self.occlusion_time == 0:\n",
    "            x[:,(50-self.occlusion_time):(50+self.occlusion_time),:,:] = 0\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "2fvDhK2n8wz_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(self, model_stream, module):\n",
    "        super(Mask, self).__init__()\n",
    "        self.model_stream = model_stream\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, weight, feature):\n",
    "        result = []\n",
    "        for i in range(self.model_stream):\n",
    "            temp_result = self.CAM(weight[i], feature[i])\n",
    "            result.append(temp_result)\n",
    "        for i in range(1, self.model_stream):\n",
    "            for j in range(i):\n",
    "                if j == 0:\n",
    "                    mask = result[j]\n",
    "                else:\n",
    "                    mask *= result[j]\n",
    "            mask = torch.cat([mask.unsqueeze(1)] * 4, dim=1)\n",
    "            print(f'mask shape: {mask[-1].shape}')\n",
    "            self.module.mask_stream[i].data = mask.view(-1).detach()\n",
    "            print(f'mask output shape: {self.module.mask_stream[i].data.shape}')\n",
    "\n",
    "    def CAM(self, weight, feature):\n",
    "        N, C = weight.shape\n",
    "        print(f'weight shape: {weight.shape}, feature shape: {feature.shape}')\n",
    "        weight = weight.view(N, C, 1, 1, 1).expand_as(feature)\n",
    "        result = (weight * feature).sum(dim=1)\n",
    "        result = result.mean(dim=0)\n",
    "\n",
    "        T, V, M = result.shape\n",
    "        result = result.view(-1)\n",
    "        result = 1 - F.softmax(result, dim=0)\n",
    "        result = F.threshold(result, 0.1, 0)\n",
    "        result = result.view(T, V, M)\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "iVmFEf9MM9M6"
   },
   "outputs": [],
   "source": [
    "def get_weights(model_stream, model, y=None):\n",
    "    W = []\n",
    "    for i in range(model_stream):\n",
    "        temp_W = model.module.stgcn_stream[i].fcn.weight\n",
    "        if y is not None:\n",
    "            temp_W = temp_W[y,:]\n",
    "        W.append(temp_W.view(temp_W.shape[0], -1))\n",
    "    return W\n",
    "\n",
    "\n",
    "# Learning Rate Adjusting\n",
    "def adjust_lr(epoch, adjustLR, optimizer):\n",
    "    # LR decay\n",
    "    if epoch in adjustLR:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] /= 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "eWrHlemc8w0B"
   },
   "outputs": [],
   "source": [
    "def train(epoch, epochs, model, train_loader, device, optimizer, loss_func, model_stream, mask_func):\n",
    "    acc, num_sample = 0, 0\n",
    "    model.module.train()\n",
    "    for num, (x, _, y, _) in enumerate(train_loader):\n",
    "\n",
    "        # Using GPU if possible\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Calculating Output\n",
    "        out, feature = model(x)\n",
    "\n",
    "        # update mask matrices\n",
    "        weight = get_weights(model_stream, model, y=y)\n",
    "        # TODO make the mask function work\n",
    "        mask_func(weight, feature)\n",
    "\n",
    "        # Calculating Loss\n",
    "        loss = loss_func(out, y)\n",
    "\n",
    "        # Loss Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculating Accuracies\n",
    "        pred = out.max(1, keepdim=True)[1]\n",
    "        acc += pred.eq(y.view_as(pred)).sum().item()\n",
    "        num_sample += x.shape[0]\n",
    "\n",
    "        # Print Loss\n",
    "        print('Epoch: {}/{}, Batch: {}/{}, Loss: {:.4f}'.format(epoch+1, epochs, num+1, len(train_loader), loss))\n",
    "\n",
    "    return acc / num_sample * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "fpn2l48o8w0B"
   },
   "outputs": [],
   "source": [
    "def eval(model, eval_loader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        acc, num_sample = 0, 0\n",
    "        for num, (x, _, y, _) in enumerate(eval_loader):\n",
    "\n",
    "            # Using GPU\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Calculating Output\n",
    "            out, _ = model(x)\n",
    "\n",
    "            # Calculating Accuracies\n",
    "            pred = out.max(1, keepdim=True)[1]\n",
    "            acc += pred.eq(y.view_as(pred)).sum().item()\n",
    "            num_sample += x.shape[0]\n",
    "\n",
    "            # Print Progress\n",
    "            print('Batch: {}/{}'.format(num+1, len(eval_loader)))\n",
    "\n",
    "    return acc / num_sample * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pfj1Zm128w0C",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "daae0559-69c4-403e-9d34-eea76d247af3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "spatial gc layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 128, 90, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 128, 45, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 128, 45, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 128, 45, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 128, 45, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 128, 45, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 256, 45, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 256, 23, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 256, 23, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 256, 23, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 256, 23, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 256, 23, 17])\n",
      "ST-GCN layer, x shape: torch.Size([16, 19]), feature shape: torch.Size([16, 256, 23, 17, 1])\n",
      "spatial gc layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 128, 90, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 128, 45, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 128, 45, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 128, 45, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 128, 45, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 128, 45, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 256, 45, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 256, 23, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 256, 23, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 256, 23, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 256, 23, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 256, 23, 17])\n",
      "ST-GCN layer, x shape: torch.Size([16, 19]), feature shape: torch.Size([16, 256, 23, 17, 1])\n",
      "spatial gc layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 64, 90, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 128, 90, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 128, 45, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 128, 45, 17])\n",
      "st-gcn layer, x shape: torch.Size([16, 128, 45, 17])\n",
      "spatial gc layer, x shape: torch.Size([16, 128, 45, 17])\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "data_transform = True\n",
    "\n",
    "# Data Loader Setting\n",
    "num_class = 20\n",
    "data_shape = (3, 90, 17, 1)\n",
    "transform = transforms.Compose([\n",
    "    Data_transform(data_transform=data_transform),\n",
    "    Occlusion_part([]),\n",
    "    Occlusion_time(0),\n",
    "])\n",
    "\n",
    "train_dataset = NTU('/content/drive/MyDrive/HRI_gestures/skeletons/', 'train', 0.8, data_shape, transform=transform)\n",
    "eval_dataset = NTU('/content/drive/MyDrive/HRI_gestures/skeletons/', 'eval', 0.8, data_shape, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                               batch_size=16, num_workers=2*len([0]),\n",
    "                               pin_memory=True, shuffle=True, drop_last=True)\n",
    "eval_loader = DataLoader(eval_dataset,\n",
    "                              batch_size=16, num_workers=2*len([0]),\n",
    "                              pin_memory=True, shuffle=False, drop_last=False)\n",
    "\n",
    "graph = Graph(max_hop=2)\n",
    "A = torch.tensor(graph.A, dtype=torch.float32, requires_grad=False).to(torch.device('cpu'))\n",
    "\n",
    "model_stream = 3\n",
    "\n",
    "if data_transform:\n",
    "  data_shape = (9, 90, 17, 1)\n",
    "\n",
    "# Model\n",
    "model = RA_GCN(data_shape, num_class, A, 0.5, [5,2], model_stream).to(torch.device('cpu'))\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer Setting , lr=0.1, momentum=0.9, weight_decay=0.0001, nesterov=True\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001, nesterov=True)\n",
    "\n",
    "# Loss Function Setting\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "mask_func = Mask(model_stream, model.module)\n",
    "\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_lr(epoch, [10,30], optimizer)\n",
    "\n",
    "    t_acc = train(epoch, num_epochs, model, train_loader, torch.device('cpu'), optimizer, criterion, model_stream, mask_func)\n",
    "    v_acc = eval(model, eval_loader, torch.device('cpu'))\n",
    "\n",
    "    train_acc.append(t_acc)\n",
    "    val_acc.append(v_acc)\n",
    "\n",
    "    # Log training/validation loss for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_acc[epoch]:.4f}, Validation Accuracy: {val_acc[epoch]:.4f}\")\n",
    "\n",
    "    print(f'============================================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3sIFqyG8w0D"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
